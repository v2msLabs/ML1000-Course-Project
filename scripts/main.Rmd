---
title: Credit Card Default Research
author:
  - name: Sumaira Afzal
    affiliation: York University School of Continuing Studies
  - name: Viraja Ketkar
    affiliation: York University School of Continuing Studies
  - name: Murlidhar Loka
    affiliation: York University School of Continuing Studies
  - name: Vadim Spirkov
    affiliation: York University School of Continuing Studies
abstract: >
  Credit card default might very well be a life altering event. It happens when a client have become severely delinquent on his/her credit card payment. It's a serious credit card status that not only affects person's standing with that credit card issuer, but also individual's credit standing in general and his/her ability to get approved for credit cards, loans, and other credit-based services. This research will make yet another attempt to predict if a client goint to default on the next payment. Employing verious machine learning technique we also will make an attemt to estime the amount a client would be able to pay when the bill comes. The authors of this study will try to discover who is more likely to default on the payment.

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---



```{r echo=FALSE, message=FALSE, warnings=FALSE}
# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=T))
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(corrplot) # correlation matrix plotting/printing
library(pROC) # to measure model performance
library(party) # classification tree
library(klaR) # naive bayes
library(caret) # predictive models
library(factoextra) # advanced plots
library(summarytools) # nicer data summary stats
library(xtable) # nice table formats
library(reshape) # dataframe transformation
library(corrplot) # nice correlation plots
library(cluster) # unsupervised learning lib
library(randomForest)

## ggplot default color palette
ggplotColours <- function(n = 15, h = c(0, 360) + 15){
  if ((diff(h) %% 360) < 1) h[2] <- h[2] - 360/n
  hcl(h = (seq(h[1], h[2], length = n)), c = 100, l = 65)
}

# set summarytools global parameters
st_options(plain.ascii = F,       # This is very handy in all Rmd documents
      style = "grid",        # This too
      footnote = NA,             # Avoids footnotes which would clutter the result
      subtitle.emphasis = F,  # This is a setting to experiment with - according to
      dfSummary.graph.col = F,
      tmp.img.dir = "~"
)

# pick palettes
mainPalette = ggplotColours()
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H', echo = T,comment = NA, prompt = F, 
                      cache = F, warnings = F, message = F,  fig.align="center")

# set xtable properties for the project
options(xtable.floating = T)
options(xtable.timestamp = "")
options(xtable.comment = F)
```


## Background

Overdepandance on credit card debt has been an ongoing theme in many countries around the word. For example US consumers started 2018 owing more than $1 trillion in credit card debt (Ref: \cite{wallethub}). It is projected that by the end of 2019 US consumers will increase their collective debt by another 60 billion dollars. Unfortunately many consumers overestimate their ability to pay the debt on time, or the unforeseen circumstances and luck of savings make people default on their payments. This is the least desirable outcome for all parties. Unpaid debt leads, in most cases, to default on the whole outstanding balance causing financial loss for the credit institutions. Majority of the clients go through tremendous emotional and financial stress, risking their credibility. The financial institution make significant efforts to evaluate the prospective client ability to sustain the debt and pay in time to avoid the credit default.  


## Objective

This study pursues two goals. The main objective is to predict as accurate as possible if the client makes the next month payment or defaults. We will employ CRISP-DM methodology (Ref: \cite{mining}) and supervised learning approach, to achieve this goal.

We are also motivated to profile, if possible, the customer base. The questions we will try to address are:

* If there are specific groups of cardholders that share similar features or behavioral patterns
* How theses groups, if exist, pay their debt

To achieve the second goal we will use unsupervised learning approach.

# Data Analysis

This research employs the data set sourced from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). This real-life data comprises 30000 observations of the credit card payment history of Taiwanese consumers.


## Data Dictionary

Column Name            | Column Description  
-----------------------| --------------------------------------------------------------------------  
ID                     | Customer ID
LIMIT_BAL              | Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit
SEX                    | Gender (1 = male; 2 = female).
EDUCATION              | Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)
MARRIAGE               | Marital status (1 = married; 2 = single; 3 = divorced; 0 - other)
AGE                    | Age (year)
PAY_1                  | PAY_1 - PAY_6 are payment statuses over a course of the last six months, where -2: Balance paid in full and no transactions this period (we may refer to this credit card account as having been 'inactive' this period). -1: Balance paid in full, but account has a positive balance at end of period due to recent transactions for which payment has not yet come due; 0: Customer paid the minimum due amount, but not the entire balance. I.e., the customer paid enough for their account to remain in good standing, but did revolve a balance. Positive numbers denote payment delay in months. For example 1 = payment delay for one month; 2 = payment delay for two months; . . .; 9 = payment delay for nine months and above. PAY_1  - Payment status in September
PAY_2                  | Payment status in August
PAY_3                  | Payment status in July
PAY_4                  | Payment status in June
PAY_5                  | Payment status in May
PAY_6                  | Payment status in April
BILL_AMT1              | BILL_AMT1 - BILL_AMT6 are bill amounts (NT dollar) from April till September. BILL_AMT1: September bill 
BILL_AMT2              | August bill
BILL_AMT3              | July bill
BILL_AMT4              | June bill
BILL_AMT5              | May bill
BILL_AMT6              | April bill
PAY_AMT1               | Amount of previous payment (NT dollar). PAY_AMT1: paid in September (August bill)
PAY_AMT2               | Amount paid in August (July bill)
PAY_AMT3               | Amount paid in July (June bill)
PAY_AMT4               | Amount paid in June (May bill)
PAY_AMT5               | Amount paid in May (April bill)
PAY_AMT6               | Amount paid in April (March bill)
**DEFAULT**            | **Target label that denotes whether the client paid the next month bill (0) or did not (1)** 


## Statistics

We start our research with the feature exploration and understanding. 

```{r dataset_summary, results="asis", echo = FALSE}
original = read.csv("../data/default-cc.csv", header = T, 
                na.strings = c("NA","","#NA"),sep=",")
print(dfSummary(original, valid.col = F, max.distinct.values = 3, heading = F, method = "render"),
       caption = "\\label{tab:dataset_summary} Credit Card Payment Data Summary", scalebbox = .8)
```

Table \ref{tab:dataset_summary} describes main statistical parameters of each column. It also outputs the values of the binary features. The first thing that jumps at us is that the data set has no missing data! We shall note that our target feature is not balanced. Almost **80%** of the clients do pay on time. Secondly female customers make **60%** of the data set. **Customer ID** column, as usual, will be dropped since it presents no analytical value. Here is a look at the data sample.

```{r data_head, results='asis', echo=FALSE}
print(xtable(original[1:12,1:15]), include.rownames = F, scalebox=.6)
print (xtable(original[1:12,13:23],
  caption = "\\label{tab:dataset_head} Credit Card Payment Data Sample"), include.rownames = F,
  scalebox = .6)
```

Let's review demographic characteristics of the customer base, namely: *EDUCATION*, *MARITAL STATUS* and *AGE*. We immediately can observe some deficiencies in the data quality (Figure: \ref{fig:demographics}). As we see the majority of the credit card holders have a university degree. There are three groups which are not supposed to be in the data set: **Unknown** - code **0**, **Unknown5** - code **5** and **Unkown6** - code **6**. We will assign these customers to the **Other** group, since the description for the aforementioned codes is not provided. 

Number of single people is slightly higher than the number of the married ones. 

Majority of the credit card holders are people between age of 25 and 50, which does not come as a surprise (Figure: \ref{fig:demographics})... Let's see if the *AGE* feature has outliers.
```{r}
print(original %>% filter(AGE < 18 || AGE > 100) %>% summarise(COUNT = n()))
```
The *AGE* feature maintains perfect data.

```{r demographics, fig.align="center", fig.cap="Customer Demographics", fig.height=8, echo=FALSE}
education = original %>%
  group_by(EDUCATION = factor(EDUCATION, labels = c("Unkown","Graduate School","University","High School","Other","Unknown 5","Unknown 6")))  %>%
  summarise(COUNT = n())
p1 = ggplot(education,aes(x=EDUCATION, y = COUNT, fill = EDUCATION)) + 
      geom_bar(stat = "identity", alpha = 0.7) +  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank())

marriage = original %>%
  group_by(MARRIAGE = factor(MARRIAGE, labels = c("Other","Married","Single","Divorsed")))  %>%
  summarise(COUNT = n())
p2 = ggplot(marriage,aes(x=MARRIAGE, y = COUNT, fill = MARRIAGE)) + 
      geom_bar(stat = "identity", alpha = 0.7) +  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank())

p3 =  ggplot(original,aes(x=AGE)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[2], colour=mainPalette[2]) 
grid.arrange(p1,p2,p3)
```

The next group of features we are going to explore is payment statuses. As per the data dictionary the payment statuses are supposed to have the status codes in the following range: **-2 : 9**. Let's verify the data integrity of the features.


```{r payment_status, fig.align="center", fig.cap="Customer Payment Statuses for the Last Six Months", echo=FALSE}
p1 =  ggplot(original,aes(x=PAY_1)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[1], colour=mainPalette[1])
p2 = ggplot(original,aes(x=PAY_2)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[2], colour=mainPalette[2])
p3 = ggplot(original,aes(x=PAY_3)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[3], colour=mainPalette[3])
p4 = ggplot(original,aes(x=PAY_4)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[4], colour=mainPalette[4])
p5 = ggplot(original,aes(x=PAY_5)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[5], colour=mainPalette[5])
p6 = ggplot(original,aes(x=PAY_6)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[6], colour=mainPalette[6])
grid.arrange(p1,p2,p3,p4,p5,p6)
```
As we have already noticed many of the credit card holders pay duly, codes: *-2 and -1* (see Figure: \ref{fig:payment_status}). Majority of the customers do maintain good standing. Noticeably though they **paid the required minimum or more but not the full balance** (code *0*). There is a rather significant group that falls behind with the payment by one or two months.
The next group of features are the bill amounts for the last six months.
```{r bill_density, fig.align="center", fig.cap="Customer Bill Amount Distribution for the Last Six Months", echo=FALSE, fig.height=10}
p1 = ggplot(original, aes(x=BILL_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p2 = ggplot(original, aes(x=BILL_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p3 = ggplot(original, aes(x=BILL_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p4 = ggplot(original, aes(x=BILL_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p5 = ggplot(original, aes(x=BILL_AMT5)) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p6 = ggplot(original, aes(x=BILL_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```

The bill payment amounts have negative values, significant amounts that reach at times **thens of thousands** of NT dollars! The negative amount on the credit card bill statements happen when a card holder overpaid his/her bill or were issued a credit after he/she already paid the bill. 

Noticeably the bill amounts have very long tails. They average in tens of thousands of TN dollars, hovering around 50,000 dollars or so on average (see Table: \ref{tab:dataset_summary}). Thus it makes the negative bill amounts we previously observed more plausible.

The last group of the features is the client monthly payments. The data maintained in those columns appears to be integral (see Table: \ref{tab:dataset_summary}). Let's see how the customer payments are distributed. We employ normal and log-scaled visualization for better presentation. 
```{r payment_density, echo=FALSE, fig.align="center", fig.cap="Customer Payment Amount Distribution for the Last Six Months", warning=FALSE, fig.height=8}
p1 = ggplot(original, aes(x=PAY_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p11 = ggplot(original, aes(x=PAY_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p2 = ggplot(original, aes(x=PAY_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p22 = ggplot(original, aes(x=PAY_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p3 = ggplot(original, aes(x=PAY_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p33 = ggplot(original, aes(x=PAY_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p4 = ggplot(original, aes(x=PAY_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p44 = ggplot(original, aes(x=PAY_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p5 = ggplot(original, aes(x=PAY_AMT5)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p55 = ggplot(original, aes(x=PAY_AMT5)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p6 = ggplot(original, aes(x=PAY_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)  +
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p66 = ggplot(original, aes(x=PAY_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
grid.arrange(p1,p11,p2,p22,p3,p33,p4,p44,p5,p55,p6,p66, ncol = 2)
```

The pay amounts mirror in the distribution the bill amounts, which is expected. The charts have very long tails which imply that the amounts the card holders pay, very greatly. Most likely the payments that are way outside of the normal distribution curve are lump sum payments. More often than not the clients pay between 1000 and 10000 dollars monthly, which is still way below the average bill amount. This finding and the payment status statistics (see Figure: \ref{fig:payment_status}) make us believe that the majority of the credit card holders do have quite significant debt, despite the good standing. This hypotheses also explains the distribution of the payments. To keep the debt growth in check the customers pay lump sums whenever they accumulate some saving. Let's plot the delta between the bill amounts and the payment amounts to support our theory. Figure \ref{fig:payment_delta_density}
```{r payment_delta_density, echo=FALSE, fig.align="center", fig.cap="Customer Bill/Payment Amount Delta for Six Months", warning=FALSE}
tmp = original %>% mutate(BILL_PAY_DELTA1 = BILL_AMT1 - PAY_1,BILL_PAY_DELTA2 = BILL_AMT2 - PAY_2,BILL_PAY_DELTA3 = BILL_AMT3 - PAY_3,BILL_PAY_DELTA4 = BILL_AMT4 - PAY_4,BILL_PAY_DELTA5 = BILL_AMT5 - PAY_5, BILL_PAY_DELTA6 = BILL_AMT6 - PAY_6 ) %>% dplyr::select(BILL_PAY_DELTA1,BILL_PAY_DELTA2,BILL_PAY_DELTA3,BILL_PAY_DELTA4,BILL_PAY_DELTA5, BILL_PAY_DELTA6)
tmp = melt(tmp)

ggplot(tmp, aes(x=value, fill=variable, colour = variable)) + 
  geom_density(alpha = 0.1)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
rm(p1,p2,p3,p4,p5,p6,p11,p22,p33,p44,p55,p66,tmp)
```

### Data Transformation

Before we proceed further we are going to clean the data set as described in the previous paragraph, namely:

* We will remove *Customer ID* column
* We assign code **4** - **Other** to the *EDUCATION* column values that fall out of the declared code range (1:4)
```{r}
original$EDUCATION = with(original, ifelse(EDUCATION == 0 | EDUCATION == 5 | EDUCATION == 6 , 4, EDUCATION))
data = dplyr::select(original, -ID)
data %>% filter(EDUCATION == 0 | EDUCATION >4) %>% summarise(COUNT=n())
```


## Data Correlation and Principal Component Analysis

In this section of our study we continue exploring the relations between various features of the data set. We put stress on finding the correlated features, the correlation between the features and the target label. We also are going to apply principal component analysis (PCA) to understand which attributes of the data set explain most variance of the original data (Ref: \cite{pca}). If our findings are fruitful we may design a model that requires smaller number of the input parameters without sacrificing the predictive power of the model.

Let's plot the correlation matrix first.
```{r corr_plot, echo=FALSE, fig.align="center", fig.cap="Data Correlation"}
corrplot(cor(select_if(data, is.numeric), use="pairwise.complete.obs"),
         method="color", type="upper",order="hclust", col = mainPalette, number.cex = .4, tl.cex=0.5,
         addCoef.col = "black", tl.col="black", tl.srt=45,sig.level = 0.4, insig = "blank", diag=FALSE )
```

The correlation matrix does not yield any surprises (see Figure: \ref{fig:corr_plot})). Bill payment amounts exhibit higher correlation as well as the payment status group. This does not give us much. The target label has no correlation with any other feature. We proceed with the PCA analysis now. We scale and center the data to achieve meaningful result. We also remove the target feature from the PCA computation. We are going to retain the 15 top components.

```{r pca, echo=FALSE}
dataNOL = dplyr::select(data,-DEFAULT)
dataNOL.pca = prcomp(dataNOL, center = T, scale. = T,  rank. = 15)
print(get_eigenvalue(dataNOL.pca))
```

Good news! The **top 10 components explain 83%** of the data variance. Let's review what the top four components are made of.

```{r ind_contr, echo=FALSE, fig.align="center", fig.cap="Feature Contribution to the Top Four Components", warning=FALSE}
p1 = fviz_contrib( dataNOL.pca, choice = "var", axes = 1, title = "Comp 1", fill = mainPalette[1], color = mainPalette[1])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p2 = fviz_contrib( dataNOL.pca, choice = "var", axes = 2, title = "Comp 2", fill = mainPalette[2], color = mainPalette[2])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p3 = fviz_contrib( dataNOL.pca, choice = "var", axes = 3, title = "Comp 3", fill = mainPalette[3], color = mainPalette[3])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p4 = fviz_contrib( dataNOL.pca, choice = "var", axes = 4, title = "Comp 4", fill = mainPalette[4], color = mainPalette[4]) + theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
grid.arrange(p1,p2,p3,p4)
```

The red dashed line on the graph (see Figure: \ref{fig:ind_contr}) indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would about 4.3%. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component. The PCA analysis supports the correlation matrix (Figure: \ref{fig:corr_plot}) in a sense that the bill amounts and the pay statuses are highly correlated and are subject to the dimentionality reduction due to the redundancy. So as we can see the first component is largely being dominated by the bill amounts and pay statuses. The second one mainly includes the payment status features and the credit card limit. The demographic features and the amount paid dominate the third and fourth components. 

It would be very interesting to see how the top two components look like in the context of the target label. Very well, Figure: \ref{fig:ind_label} shows that there is no clear separation between component one and two. As the previous chart highlighted (see Figure: \ref{fig:ind_contr}) this shall be expected since the top two components largely comprise bill payments and pay statuses. We also observe two linear formations: one is located in the first quadrant; green (no default). The second formation is located in the fourth quadrant; it peppered with the red color that denotes the default green. to understand better what those formations are we will plot the feature vectors in the context of the component one and two.

```{r vars, echo=FALSE, fig.align="center", fig.cap="Feature contribution to the Top Two Componenets", warning=FALSE, out.width = "80%"}
fviz_pca_ind(dataNOL.pca,geom.ind = "point", col.ind = as.factor(data$DEFAULT), alpha.ind = 0.7,
             title = "", palette = c(mainPalette[6],mainPalette[1]), addEllipses = T,
             legend.title = "Default (Y-1/N-0)")
```
 
```{r ind_label, echo=FALSE, fig.align="center", fig.cap="Components and Feature Contribution in the Context of Target Label", warning=FALSE, out.width = "80%"}
fviz_pca_var(dataNOL.pca,col.var = "coord", gradient.cols = mainPalette,
             repel =T, legend.title = "Contribution", title ="")
```
The plot submitted above clearly shows that the biggest contributor to the first quadrant is a bill amount group. And the contributors to the fourth quadrant are payments statuses.

## Takeaways from Data Exploration Excersize  

We have concluded the data exploration study. There are a few major points we would like to highlight. They are:

* The majority of the credit card holders maintain good standing (see Figure: \ref{fig:payment_status}). 
* The clients do maintain debt. On regular basis they pay the amount that meets the minimal required payment but less then the bill amount (see Figure \ref{fig:payment_delta_density}). 
* The clients tend to reduce the amount of debt paying the outstanding balance in lump sums (Figure: \ref{fig:payment_density}).
* Demographically married and single people represented almost equally, women are represented better than men. Majority of the credit card holders have a university degree. The vast majority of the clients are between 25 and 50 year old.
* The data set is not balanced; the number of the customers in a good standing is much higher than the number of people who defaulted on the next payment.
* Principal component analysis showed that we can reduce the number of the features to 10 - 12 sustaining the data variance coverage at about **85%**. The top principal components comprise mainly bill amounts and pay statuses (Figure: \ref{fig:ind_contr}). The components do not have clear separation and affect equally the target label (Figure: \ref{fig:vars}).


# Classification Models and Model Performance Evalutation

In this section we will try reach our main goal - predict if the cardholder is going to default on the next payment or not.We will evaluate three supervised learning approaches: Naive Bayes, Random Forest and Logistic Regression (Ref: \cite{caret}). Each model will be cross-validated employing k-fold technique. We will analyse the ROC curve and confusion matrix of each model.

In addition to the said above we compare performance of the logistic regression algorithm on the whole data set and the data set that comprises 10-top components identified during PCA analysis.

In the end of this section we plot all tree model AUC stats.

## Data Preparation

Prior to fitting the models with the data we would have to upsample the training set, because our data set is not balanced; the number of customers who defaults is much smaller than the number of the paying customers.

All models we are going to fit will benefit from the data scaling and centering, thus we are going to apply these transformations. Below numbers are the count of the deaults on the balanced data set.

```{r, echo=FALSE}
splitIdx = caret::createDataPartition(y=data$DEFAULT, p=0.7, list=FALSE)
trainData = data[splitIdx,]
testData  = data[-splitIdx,]

# sample. REMOVE for final run
#trainData = sample_n(trainData, 2000)
#testData = sample_n(testData,2000)

preprocParams = c("BoxCox", "center", "scale");

columns = colnames(trainData)
trainData = upSample(x = trainData[, columns[columns != "DEFAULT"] ], 
      y = as.factor(trainData$DEFAULT), list = F, yname = "DEFAULT")
print(table(trainData$DEFAULT))
```

## Feature Selection

We will be using all features of the data set. In the case of the logistic regression we will examine the algorithm performance on the PCA-reduced data set. 

## Naive Bayes Model

Naïve Bayes classification is a kind of simple probabilistic classification methods based on Bayes’ theorem with the assumption of independence between features. 

It is simple (both intuitively and computationally), fast, performs well with small amounts of training data, and scales well to large data sets. The greatest weakness of the Naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities. Although this assumption is rarely met, in practice, this algorithm works surprisingly well and accurate; however, on average it rarely can compete with the accuracy of advanced tree-based methods (random forests & gradient boosting machines) but is definitely worth having in our toolkit.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# possible tuning grid
# tuninGrid = data.frame(fL=c(0,0.5,1.0), usekernel = TRUE, adjust=c(0,0.5,1.0))
set.seed(876)
trainDataCopy = mutate(trainData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))
testDataCopy = mutate(testData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))
ctrl = trainControl(method="cv", number = 5, 
    # Estimate class probabilities
    classProbs = T,
    # Evaluate performance using the following function
    summaryFunction = twoClassSummary)

naiveBayesModel = caret::train(DEFAULT ~ ., preProcess = preprocParams,
   data = trainDataCopy, method = "nb", metric="ROC", trControl = ctrl)

pred.naiveBayesModel.prob = predict(naiveBayesModel, newdata = testDataCopy, type="prob")
pred.naiveBayesModel.raw = predict(naiveBayesModel, newdata = testDataCopy )

roc.naiveBayesModel = pROC::roc(testDataCopy$DEFAULT, 
                     as.vector(ifelse(pred.naiveBayesModel.prob[,"yes"] >0.5, 1,0)) )
auc.naiveBayesModel = pROC::auc(roc.naiveBayesModel)
naiveBayesModel
```
```{r echo=FALSE}
confusionMatrix(data = pred.naiveBayesModel.raw, testDataCopy$DEFAULT)
```
```{r plot_nb_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Naive Bayes Model AUC and ROC Curve", out.width="70%"}
plot.roc(roc.naiveBayesModel, print.auc = T, auc.polygon = T, col = mainPalette[2] , print.thres = "best" )
```

## Random Forest Model
Random Forest is also considered as a very handy and easy to use algorithm, because it’s default hyper parameters often produce a good prediction result. Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. The main limitation of Random Forest is that a large number of trees can make the algorithm to slow and ineffective for real-time predictions. In general, these algorithms are fast to train, but quite slow to create predictions once they are trained.
```{r echo=FALSE, message=FALSE, warning=FALSE}

trainDataCopy = mutate(trainData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))
testDataCopy = mutate(testData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))
ctrl = trainControl(method = "cv", number = 3,  
    # Estimate class probabilities
    classProbs = T,
    # Evaluate performance using the following function
    summaryFunction = twoClassSummary)

randomForestModel = caret::train(DEFAULT ~ ., preProcess = preprocParams,
   data = trainDataCopy, method = "rf", metric="ROC", trControl = ctrl)

pred.randomForestModel.prob = predict(randomForestModel, newdata = testDataCopy, type="prob")
pred.randomForestModel.raw = predict(randomForestModel, newdata = testDataCopy )

roc.randomForestModel = pROC::roc(testDataCopy$DEFAULT,  
                                  as.vector(ifelse(pred.randomForestModel.prob[,"yes"] >0.5, 1,0)) )
auc.randomForestModel = pROC::auc(roc.randomForestModel)
randomForestModel
```
```{r, echo=FALSE}
confusionMatrix(data = pred.randomForestModel.raw, testDataCopy$DEFAULT)
```
```{r plot_rf_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Random Forest Model AUC and ROC Curve", out.width="70%"}
plot.roc(roc.randomForestModel, print.auc = T, auc.polygon = T, col = mainPalette[3] , print.thres = "best" )
```


## Logistic Regression Model
Logistic regression is an efficient, interpretable and accurate method, which fits quickly with minimal tuning. Logistic regression prediction accuracy will benefit if the data is close to Gaussian distribution. Thus we apply addition transformation to the training data set. We will also be employing 5-fold cross-validation re-sampling procedure to improve the model.
```{r echo=FALSE, message=FALSE, warning=FALSE}
trainDataCopy = mutate(trainData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))
testDataCopy = mutate(testData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes")))

set.seed(123)

ctrl = trainControl(
  # 5-fold CV
  method="cv", number = 5,  
  savePredictions = T)

logRegModel = caret::train(DEFAULT ~ ., preProcess = preprocParams,
        data = trainDataCopy, method="glm", family = binomial(link = "logit"), 
        trControl = ctrl)

pred.logRegModel.raw = predict(logRegModel, newdata =  testDataCopy)
pred.logRegModel.prob = predict(logRegModel, newdata =  testDataCopy, type = "prob")

roc.logRegModel = pROC::roc(testDataCopy$DEFAULT, as.vector(ifelse(pred.logRegModel.prob[,1] >0.5, 1,0)))
auc.logRegModel = pROC::auc(roc.logRegModel)
logRegModel
```
```{r, echo=FALSE}
confusionMatrix(data = pred.logRegModel.raw, testDataCopy$DEFAULT)
```

```{r plot_logReg_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Logistic Regression Model AUC and ROC Curve", out.width="70%"}
plot.roc(roc.logRegModel, rint.auc = T, auc.polygon = T, col = mainPalette[4] , print.thres = "best" )
```

Confusion matrix and Figure \ref{fig:plot_logReg_ROC} demonstrate the logistic model performance on the balanced data set. Using the proportion of positive data points that are correctly considered as positive (true positives) and the proportion of negative data points that are mistakenly considered as positive (false negative), we generated a graphic that shows the trade off between the rate at which the model correctly predicts the rain tomorrow with the rate of incorrectly predicting the rain. The value around 0.67 indicates that the model does a good job in discriminating between the two categories.

Let's now compare the performance of the PCA components to the performance of the full data set. Just to remind we selected have identified 10 top components that explain *83%* of data variance. We are going to reduce the dimentionality of the training and test data set multiplying them on the principal component weight matrix.

```{r echo=FALSE, warning=FALSE}
trainClass = mutate(trainData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes"))) %>% dplyr::select(DEFAULT)
trainDataCopy =trainData %>% dplyr::select(-DEFAULT)
testClass = mutate(testData, DEFAULT = as.factor(ifelse(DEFAULT==0, "no", "yes"))) %>% dplyr::select(DEFAULT)
testDataCopy = testData %>% dplyr::select(-DEFAULT)

comps = dataNOL.pca$rotation[,1:10]

set.seed(15623)

testDataReduced =  data.frame( as.matrix(testDataCopy) %*% comps)
trainDataReduced =  data.frame( as.matrix(trainDataCopy) %*% comps)

logRegPCAModel = caret::train(x = trainDataReduced, y = trainClass$DEFAULT, method = "glm", trControl = ctrl, family = binomial(link = "logit"), preProcess = preprocParams)

pred.logRegPCAModel.raw = predict(logRegPCAModel, newdata = testDataReduced )
confusionMatrix(data = pred.logRegPCAModel.raw, testClass$DEFAULT)
```

Well the result of the model fitted with the PCA components is inferior to the same very model. The balanced accuracy of the data set with lower dimentionality dropped by about **7%**. But we shall not forget that the PCA components data set had less than a half of the features of the original data set. Tolerance of the reduction of the predictive power of the model depends on the business requirements. In some cases it might be Okay in others not. In our particular case we consider such precision drop is critical, thus we stick to the fully featured data set.  


## Model Comparison
Now it is time to compare the models side by side and pick a winner.

```{r plot_model_comp, fig.align="center", fig.cap="Model AUC Comparison", message=FALSE, warning=FALSE, echo=FALSE, out.width="70%"}
modelsFace2Face = data.frame(model=c("logRegModel", "naiveBayesModel", "randomForestModel"),
        auc=c(auc.logRegModel, auc.naiveBayesModel, auc.randomForestModel))
modelsFace2Face = modelsFace2Face[order(modelsFace2Face$auc, decreasing = T),]
modelsFace2Face$model = factor(modelsFace2Face$model, levels = modelsFace2Face$model)

ggplot(data = modelsFace2Face, aes(x=model, y=auc)) +
  geom_bar(stat="identity", fill=mainPalette[7], colour=mainPalette[7], alpha = 0.5)

print(modelsFace2Face)
```

### AUC - ROC perfomance

AUC stands for Area under the ROC Curve and ROC for Receiver operating characteristic curve. This is one of the most important KPIs of the classification algorithms. These two metrics measure how well the models distinguishing between the classes. The higher AUC the better model predicts positive and negative outcome. 

Figures  \ref{fig:plot_nb_ROC}, \ref{fig:plot_rf_ROC}, \ref{fig:plot_logReg_ROC} and accompanying data show that on the test data set all the models demonstrated very close results. Random Forest has the highest overall accuracy (~80%) but performs poorly on the negative outcome - *paymant default* (43%), thus the balanced accuracy is lower (about 67%). 

Naive Bayes model almost mirrors the random forest in the overall accuracy.But its specificity is even lower, 28%. The balanced accuracy of the model is about 62%.

Logistic regression model is the most blanaced one; it separates positive and negative outcomes equally well. It's balanced accuracy is **67%**.


### Model Interpretibility

Logistic Regression and Naive Bayes are all highly interpretable models. It is easy to explain to the business what impact each input parameter has. The decision tree could be visualized (provided if it is not too large). 

Random Forest on the other hand is a black-box model, complex algorithm which is difficult to explain in simple terms.


### Data Pre-Processing

Random Forest and Naive Bayes can deal with missing data, outliers, numeric and alphanumeric values. Simply speaking they are not very demanding for data quality. It would be interesting to see how they perform on the original data set without data cleaning. But this is subject of another research...

Logistic regression does require conversion of alphanumeric values to numeric, struggles dealing with the outliers and performs best when fitted with the data that have normal distribution. 

### Verdict

Despite sensitivity to data quality Logistic Regression outperforms Naive Bayes and Random Forest models in the ability to separate positive and negative classes. It is fast, scales well and highly interpretable. Thus this is our winner.


# Undertanding the Client Base Employing Unsupervised Learning

Lastly we believe it would be beneficial to profile the client base. This would add additional insights into understanding of the credit card holders demographics, spending and borrowing habits. We will be employing *Clastering Large Applications (CLARA)* approach to attack this challenge. This study does not make its goal to compare various unsupervised model techniques. *CLARA* has been chosen because it is robust, relatively easy to understand, scales well, handles categorical and continuous features and fast.

It is based on The **Partitioning Around Medoids** (*PAM*) algorithm, which is a popular realization of k-medoids clustering method. We employ the silhouette coefficient measure to gauge how well the clusters are separated. The silhouette analysis also provides insights into the cluster density.

After many trials and errors we have selected a few features that describe the client financial and demographic profile quite well, namely:

* Bill amount history
* Credit standing history
* Cardholder education

Figure \ref{fig:silh} shows that the clusters are fairly well defined. There is a minor overlap between clusters 2, 3 and 4.
```{r silh,  echo=FALSE, warning=FALSE, fig.align="center", fig.cap="Cluster Silhouette",  out.width = "80%"}
dataC =  dplyr::select(data,EDUCATION,
                      BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6,
PAY_1, PAY_2, PAY_3, PAY_4,PAY_5,PAY_6)  

dataCScaled = dataC %>% scale()
clara = clara(dataCScaled, k = 4, metric = "euclidean",stand = F, samples = 100, sampsize =500)
clusteredData = cbind(data, CLUSTER=c(clara$clustering))
fviz_silhouette(clara, title = "") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

```

The plot \ref{fig:clusters} renders the clusters shape against the first two dimensions.
```{r clusters,  echo=FALSE, warning=FALSE, fig.align="center", fig.cap="Clusters",  out.width = "80%"}
fviz_cluster(clara, dataCScaled,   geom = "point", main="",
   axes = c(1,2), xlab = "Education", ylab = "Last Month Bill Amount") 
```

So what are those clusters are. How to interpret them? The next few paragraphs visualize and describe the cluster contents.

## Cluster #1 - The Regular Folks

Nothing particular stands out about this group. Majority of the clients in this group pay required minimum. small percentage of the group delay the payments but no more than 2 months. Their monthly bills rarely go beyond 75,000 Taiwanese dollars. Some members of this cluster have high credit limit, but it does not look like they take advantage of it. They are rather well educated, range between 20 and 40 years of age. This is the largest category that describes pretty accurately the main mass of the cardholder - **43.7%**. 

```{r cluster1, echo=FALSE, fig.align="center", fig.cap="Cluster One - The Regular Folks", warning=FALSE, fig.height=8}
payStatusName = c("Inactive","Paid in Full","Paid Minimum Due","Month Delay", "Two-Month Delay",
                 "Three-Month Delay","Four-Month Delay","Five-Month Delay","Six-Month Delay","Seven-Month Delay",
                 "Eight-Month Delay","Nine-Plus")
edName = c("Graduate school","University","High school","Other")
ageInt = c(20,30,40,50,60)
ageName = c("20+","30+","40+","50+","60+")
limitInt = c(50000,100000,150000,250000,350000,500000)
limitIntName = c("<$50,000","<$100,000","<$150,000","<$250,000","<$350,000","<$500,000","$500,000+")
billInt = c(15000,30000,45000,60000,75000,90000,150000)
billIntName = c("<$15,000","<$30,000","<$45,000","<$60,000","<$75,000","<$90,000","<$150,000","$150,000+")
defaultName = c("No","Yes")

clusteredData$CLIENT_STANDING = as.factor(payStatusName[3+clusteredData$PAY_1])
clusteredData$Education = as.factor(edName[clusteredData$EDUCATION])
clusteredData$AGE_GROUP= as.factor(ageName[findInterval(clusteredData$AGE,ageInt)])
clusteredData$CREDIT_LIMIT= as.factor(limitIntName[1+findInterval(clusteredData$LIMIT_BAL,limitInt)])
clusteredData$BILL_AMOUNT= as.factor(billIntName[1+findInterval(clusteredData$BILL_AMT1,billInt)])
clusteredData$DO_DEFAULT = as.factor(defaultName[1+clusteredData$DEFAULT])

c = clusteredData %>% filter(CLUSTER == 1) 
cEd = c %>% group_by(Education) %>% summarise(COUNT=n())
cAge = c %>% group_by(AGE_GROUP) %>% summarise(COUNT=n())
cLimit = c %>% group_by(CREDIT_LIMIT) %>% summarise(COUNT=n())
cBill = c %>% group_by(BILL_AMOUNT) %>% summarise(COUNT=n())
cStadning = c %>% group_by(CLIENT_STANDING) %>% summarise(COUNT=n())
cDefault  = c %>% group_by(DO_DEFAULT) %>% summarise(COUNT=n())

simple_theme = theme_minimal()+
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.border = element_blank(),
  panel.grid=element_blank(),
  axis.ticks = element_blank(),
  panel.background = element_blank(),
  legend.title=element_text(size=9), 
  legend.text=element_text(size=7)
  )

p1 =  ggplot(cBill, aes(x="", y=COUNT, fill=BILL_AMOUNT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p2 = ggplot(cStadning, aes(x="", y=COUNT, fill=CLIENT_STANDING))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p3 = ggplot(cEd, aes(x="", y=COUNT, fill=Education))+
geom_bar(stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p4 = ggplot(cAge, aes(x="", y=COUNT, fill=AGE_GROUP))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p5 = ggplot(cLimit, aes(x="", y=COUNT, fill=CREDIT_LIMIT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p6 = ggplot(cDefault, aes(x="", y=COUNT, fill=DO_DEFAULT))+
geom_bar(width = 0.7, stat = "identity" ) + scale_fill_manual(values = c(mainPalette[6], mainPalette[1])) +simple_theme

grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```

## Cluster #2 - The Exuberant Spenders

People in this cluster live the life! Their monthly bills often go beyond $150,000 mark. Many group members have a credit limit in the range 250,000 or more. As the *Regular Folks* the cardholder of this class do not pay the full bill amount. There is a good sector of the clients that are 3 month behind with the payment. Education-wise the group almost equally split between the university and graduate school graduates. Just like in the case of the first cluster majority of the people in this group are between 20 and 40 years old, where **30+** age group dominates. This is the third largest group of the credit card holder population - **14.8%**. 

```{r cluster2, echo=FALSE, fig.align="center", fig.cap="Cluster Two - The Exuberant Spenders", warning=FALSE, fig.height=8}
c = clusteredData %>% filter(CLUSTER == 2) 
cEd = c %>% group_by(Education) %>% summarise(COUNT=n())
cAge = c %>% group_by(AGE_GROUP) %>% summarise(COUNT=n())
cLimit = c %>% group_by(CREDIT_LIMIT) %>% summarise(COUNT=n())
cBill = c %>% group_by(BILL_AMOUNT) %>% summarise(COUNT=n())
cStadning = c %>% group_by(CLIENT_STANDING) %>% summarise(COUNT=n())
cDefault  = c %>% group_by(DO_DEFAULT) %>% summarise(COUNT=n())

p1 =  ggplot(cBill, aes(x="", y=COUNT, fill=BILL_AMOUNT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p2 = ggplot(cStadning, aes(x="", y=COUNT, fill=CLIENT_STANDING))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p3 = ggplot(cEd, aes(x="", y=COUNT, fill=Education))+
geom_bar(stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p4 = ggplot(cAge, aes(x="", y=COUNT, fill=AGE_GROUP))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p5 = ggplot(cLimit, aes(x="", y=COUNT, fill=CREDIT_LIMIT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p6 = ggplot(cDefault, aes(x="", y=COUNT, fill=DO_DEFAULT))+
geom_bar(width = 0.7, stat = "identity" ) + scale_fill_manual(values = c(mainPalette[6], mainPalette[1])) +simple_theme

grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```

## Cluster #3 - The Realists

The second largest group of the clients (**32.7%**) are people who really keep their spending in check. The vast majority of the people in this cluster have the bill amount less than $15,000 NT (about 650 CAD). Unlike the previous two groups almost half of the realists pay their bills in full. Only small percentage have delayed the payment by a month. The realists are generally older. Half of the group finished the graduate school, the second largest education group are university graduates. The credit limit of the group varies and could be very high but they do not fall victims to seduction!


```{r cluster3, echo=FALSE, fig.align="center", fig.cap="Cluster Three - The Realists", warning=FALSE, fig.height=8}
c = clusteredData %>% filter(CLUSTER == 3) 
cEd = c %>% group_by(Education) %>% summarise(COUNT=n())
cAge = c %>% group_by(AGE_GROUP) %>% summarise(COUNT=n())
cLimit = c %>% group_by(CREDIT_LIMIT) %>% summarise(COUNT=n())
cBill = c %>% group_by(BILL_AMOUNT) %>% summarise(COUNT=n())
cStadning = c %>% group_by(CLIENT_STANDING) %>% summarise(COUNT=n())
cDefault  = c %>% group_by(DO_DEFAULT) %>% summarise(COUNT=n())

p1 =  ggplot(cBill, aes(x="", y=COUNT, fill=BILL_AMOUNT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p2 = ggplot(cStadning, aes(x="", y=COUNT, fill=CLIENT_STANDING))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p3 = ggplot(cEd, aes(x="", y=COUNT, fill=Education))+
geom_bar(stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p4 = ggplot(cAge, aes(x="", y=COUNT, fill=AGE_GROUP))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p5 = ggplot(cLimit, aes(x="", y=COUNT, fill=CREDIT_LIMIT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p6 = ggplot(cDefault, aes(x="", y=COUNT, fill=DO_DEFAULT))+
geom_bar(width = 0.7, stat = "identity" ) + scale_fill_manual(values = c(mainPalette[6], mainPalette[1])) +simple_theme

grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```

## Cluster #4 - The Grinders 

The striking difference between this cluster of people and the others is a default rate; it is about **65%**! The majority of the grinders have relatively low bills: $45,000 or less. Smaller credit limits, - less than 100,000. Yet, predominantly they are two or more month late with their payments. This is a highly educated group, majority of which are university graduates. Wait a minute... Age-wise this is the youngest group of all, where over 40% a people in their twenties. Maybe they are still students? Thankfully this is the smallest group that make **8.6** of the total population of the cardholders. 

```{r cluster4, echo=FALSE, fig.align="center", fig.cap="Cluster Four- The Grinders", warning=FALSE, fig.height=8}
c = clusteredData %>% filter(CLUSTER == 4) 
cEd = c %>% group_by(Education) %>% summarise(COUNT=n())
cAge = c %>% group_by(AGE_GROUP) %>% summarise(COUNT=n())
cLimit = c %>% group_by(CREDIT_LIMIT) %>% summarise(COUNT=n())
cBill = c %>% group_by(BILL_AMOUNT) %>% summarise(COUNT=n())
cStadning = c %>% group_by(CLIENT_STANDING) %>% summarise(COUNT=n())
cDefault  = c %>% group_by(DO_DEFAULT) %>% summarise(COUNT=n())

p1 =  ggplot(cBill, aes(x="", y=COUNT, fill=BILL_AMOUNT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p2 = ggplot(cStadning, aes(x="", y=COUNT, fill=CLIENT_STANDING))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p3 = ggplot(cEd, aes(x="", y=COUNT, fill=Education))+
geom_bar(stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p4 = ggplot(cAge, aes(x="", y=COUNT, fill=AGE_GROUP))+
geom_bar(width = 1, stat = "identity") +   coord_polar("y", start=0) +  simple_theme

p5 = ggplot(cLimit, aes(x="", y=COUNT, fill=CREDIT_LIMIT))+
geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) +  simple_theme

p6 = ggplot(cDefault, aes(x="", y=COUNT, fill=DO_DEFAULT))+
geom_bar(width = 0.7, stat = "identity" ) + scale_fill_manual(values = c(mainPalette[6], mainPalette[3])) +simple_theme

grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```


# Model Deployment

Our best classification model has predictive power of 68%. Is it good enough? It is hard to tell. We feel that the model meets our first objective, which is prediction of the default on the next month payment. Logistic regression model we picked is fast, equally sensitive to the positive and negative outcomes. It produces reasonably good result even on the lower dimension data sets. The model is fast and easy to deploy. Due to the nature of the business the model does require frequent data updates and re-training.

We are also very satisfied with accomplishment of our second goal, which is understanding of the customer base. We believe that we pretty accurately captured the main characteristics and behavioral patterns of the credit card holders. **CLARA** approach we employed provided very plausible result. This model is fast and easy to deploy. It does not require significant computing power to produce good result.


# Conclusion

Through exploring credit card holders borrowing and spending patterns collected in 2006 in Taiwan we were able to come up with two models. One is a binary classifier, which predicts a default on the upcoming credit card bill. The second model provides in-depth view of the client base.  

The study started with thorough analysis of the data set. At this phase we were able to identified many interesting patterns that insured the success of the whole project. We commenced our research providing descriptive stats on all available features of the data set. We also applied Principal Component Analysis approach to deduce which features carried the most information.

Then we applied and evaluated three supervised learning algorithms: Logistic Regression, Naive Bayes and Random Forest. The logistic regression model we tested with the whole feature set and with the component-based set, which we picked during the PCA phase. All three models were k-fold cross-validated and thoroughly evaluate employing ROC curve and confusion matrix approaches.

Lastly we applied unsupervised learning to understand who the credit card holders are, how they borrow and spend money, whether they are the clients in good standing or prone to default. 

As a result of this study we fully understood the data we dealt with. We designed reasonably accurate credit card default prediction model. We managed to group the clients into meaningful, highly interpretable clusters that explain the customer behavioral patterns well.

Overall we believe we have achieved all our goals. 


\bibliography{RJreferences}

# Note from the Authors

This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.
