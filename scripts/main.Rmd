---
title: Credit Card Default Research
author:
  - name: Sumaira Afzal
    affiliation: York University School of Continuing Studies
  - name: Viraja Ketkar
    affiliation: York University School of Continuing Studies
  - name: Murlidhar Loka
    affiliation: York University School of Continuing Studies
  - name: Vadim Spirkov
    affiliation: York University School of Continuing Studies
abstract: >
  Credit card default might very well be a life altering event. It happens when a client have become severely delinquent on his/her credit card payment. It's a serious credit card status that not only affects person's standing with that credit card issuer, but also individual's credit standing in general and his/her ability to get approved for credit cards, loans, and other credit-based services. This research will make yet another attempt to predict if a client goint to default on the next payment. Employing verious machine learning technique we also will make an attemt to estime the amount a client would be able to pay when the bill comes. The authors of this study will try to discover who is more likely to default on the payment.

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---



```{r echo=FALSE, message=FALSE, warnings=FALSE}
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(corrplot) # correlation matrix plotting/printing
library(pROC) # to measure model performance
library(party) # classification tree
library(klaR) # naive bayes
library(caret) # predictive models
library(factoextra) # advanced plots
library(summarytools) # nicer data summary stats
library(xtable) # nice table formats
library(reshape) # dataframe transformation
library(corrplot) # nice correlation plots
library(pscl) #used when calculating pseudo R2 for logistic models

# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=T))
source('utils.R') # supplementary code

# set summarytools global parameters
st_options(plain.ascii = F,       # This is very handy in all Rmd documents
      style = "grid",        # This too
      footnote = NA,             # Avoids footnotes which would clutter the result
      subtitle.emphasis = F,  # This is a setting to experiment with - according to
      dfSummary.graph.col = F,
      tmp.img.dir = "~"
)

# pick palettes
mainPalette = ggplotColours()
# set a sample size
SampleSize = 30000
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H', echo = T,comment = NA, prompt = F, 
                      cache = F, warnings = F, message = F,  fig.align="center")

# set xtable properties for the project
options(xtable.floating = T)
options(xtable.timestamp = "")
options(xtable.comment = F)
```


## Background

Overdepandance on credit card debt has been an ongoing theme in many countries around the word. For example US consumers started 2018 owing more than $1 trillion in credit card debt (Ref: \cite{wallethub}) It is projected that by the end of 2019 US consumers will increase their collective debt by another 60 billion dollars. Unfortunately many consumers overestimate their ability to pay the debt on time, or the unforeseen circumstances and luck of savings make people default on their payments. This is the least desirable outcome for all parties. Unpaid debt leads, in most cases, to default on the whole outstanding balance causing financial loss for the credit institutions. Majority of the clients go through tremendous emotional and financial stress, risking their credibility. The financial institution make significant efforts to evaluate the prospective client ability to sustain the debt and pay in time to avoid the credit default.  


## Objective

This study pursues a few goals. First of all employing the client personal characteristics and the last six month payment history we would like to predict ax accurate as possible if the client makes the next month payment or defaults. We will employ a few supervised learning models to attack the problem. 

Another objective is to understand which features of the data set have the most impact on the next payment success/ failure.

We are also motivated to unearth, if possible, any trend that might shed light on what make people to default on the payment. And lastly the authors of this study will try to estimate how much a client could pay when the next bill comes 


# Data Analysis

This research employs the data set sourced from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). This real-life data comprises 30000 observations of the credit card payment history of Taiwanese consumers.


## Data Dictionary

Column Name            | Column Description  
-----------------------| --------------------------------------------------------------------------  
ID                     | Customer ID
LIMIT_BAL              | Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit
SEX                    | Gender (1 = male; 2 = female).
EDUCATION              | Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)
MARRIAGE               | Marital status (1 = married; 2 = single; 3 = divorced; 0 - other)
AGE                    | Age (year)
PAY_1                  | PAY_1 - PAY_6 are payment statuses over a course of the last six months, where -2: Balance paid in full and no transactions this period (we may refer to this credit card account as having been 'inactive' this period). -1: Balance paid in full, but account has a positive balance at end of period due to recent transactions for which payment has not yet come due; 0: Customer paid the minimum due amount, but not the entire balance. I.e., the customer paid enough for their account to remain in good standing, but did revolve a balance. Positive numbers denote payment delay in months. For example 1 = payment delay for one month; 2 = payment delay for two months; . . .; 9 = payment delay for nine months and above. PAY_1  - Payment status in September
PAY_2                  | Payment status in August
PAY_3                  | Payment status in July
PAY_4                  | Payment status in June
PAY_5                  | Payment status in May
PAY_6                  | Payment status in April
BILL_AMT1              | BILL_AMT1 - BILL_AMT6 are bill amounts (NT dollar) from April till September. BILL_AMT1: September bill 
BILL_AMT2              | August bill
BILL_AMT3              | July bill
BILL_AMT4              | June bill
BILL_AMT5              | May bill
BILL_AMT6              | April bill
PAY_AMT1               | Amount of previous payment (NT dollar). PAY_AMT1: paid in September (August bill)
PAY_AMT2               | Amount paid in August (July bill)
PAY_AMT3               | Amount paid in July (June bill)
PAY_AMT4               | Amount paid in June (May bill)
PAY_AMT5               | Amount paid in May (April bill)
PAY_AMT6               | Amount paid in April (March bill)
**DEFAULT**            | **Target label that denotes whether the client paid the next month bill (0) or did not (1)** 


## Data Exploration

### Feature Analytics

We start our research with the feature exploration and understanding. 

```{r dataset_summary, results="asis", echo = FALSE}
original = read.csv("../data/default-cc.csv", header = T, 
                na.strings = c("NA","","#NA"),sep=",")
print(dfSummary(original, valid.col = F, max.distinct.values = 3, heading = F, method = "render"),
       caption = "\\label{tab:dataset_summary} Credit Card Payment Data Summary", scalebbox = .8)
```

Table \ref{tab:dataset_summary} describes main statistical parameters of each column. It also outputs the values of the binary features. The first thing that jumps at us is that the data set has no missing data! We shall note that our target feature is not balanced. Almost **80%** of the clients do pay on time. Secondly female customers make **60%** of the data set. **Customer ID** column, as usual, will be dropped since it presents no analytical value. Here is a look at the data sample.

```{r data_head, results='asis', echo=FALSE}
print(xtable(original[1:12,1:15]), include.rownames = F, scalebox=.6)
print (xtable(original[1:12,13:23],
  caption = "\\label{tab:dataset_head} Credit Card Payment Data Sample"), include.rownames = F,
  scalebox = .6)
```

Let's review demographic characteristics of the customer base, namely: *EDUCATION*, *MARITAL STATUS* and *AGE*. We immediately can observe some deficiencies in the data quality (Figure: \ref{fig:demographics}). As we see the majority of the credit card holders have a university degree. There are three groups which are not supposed to be in the data set: **Unknown** - code **0**, **Unknown5** - code **5** and **Unkown6** - code **6**. We will assign these customers to the **Other** group, since the description for the aforementioned codes is not provided. 

Number of single people is slightly higher than the number of the married ones. 

Majority of the credit card holders are people between age of 25 and 50, which does not come as a surprise (Figure: \ref{fig:demographics})... Let's see if the *AGE* feature has outliers.
```{r}
print(original %>% filter(AGE < 18 || AGE > 100) %>% summarise(COUNT = n()))
```
The *AGE* feature maintains perfect data.

```{r demographics, fig.align="center", fig.cap="Customer Demographics", fig.height=8, echo=FALSE}
education = original %>%
  group_by(EDUCATION = factor(EDUCATION, labels = c("Unkown","Graduate School","University","High School","Other","Unknown 5","Unknown 6")))  %>%
  summarise(COUNT = n())
p1 = ggplot(education,aes(x=EDUCATION, y = COUNT, fill = EDUCATION)) + 
      geom_bar(stat = "identity", alpha = 0.7) +  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank())

marriage = original %>%
  group_by(MARRIAGE = factor(MARRIAGE, labels = c("Other","Married","Single","Divorsed")))  %>%
  summarise(COUNT = n())
p2 = ggplot(marriage,aes(x=MARRIAGE, y = COUNT, fill = MARRIAGE)) + 
      geom_bar(stat = "identity", alpha = 0.7) +  theme(axis.text.x=element_blank(),
      axis.ticks.x=element_blank())

p3 =  ggplot(original,aes(x=AGE)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[2], colour=mainPalette[2]) 
grid.arrange(p1,p2,p3)
```

The next group of features we are going to explore is payment statuses. As per the data dictionary the payment statuses are supposed to have the status codes in the following range: **-2 : 9**. Let's verify the data integrity of the features.


```{r payment_status, fig.align="center", fig.cap="Customer Payment Statuses for the Last Six Months", echo=FALSE}
p1 =  ggplot(original,aes(x=PAY_1)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[1], colour=mainPalette[1])
p2 = ggplot(original,aes(x=PAY_2)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[2], colour=mainPalette[2])
p3 = ggplot(original,aes(x=PAY_3)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[3], colour=mainPalette[3])
p4 = ggplot(original,aes(x=PAY_4)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[4], colour=mainPalette[4])
p5 = ggplot(original,aes(x=PAY_5)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[5], colour=mainPalette[5])
p6 = ggplot(original,aes(x=PAY_6)) + 
      geom_bar(stat = "count", alpha = 0.7, fill=mainPalette[6], colour=mainPalette[6])
grid.arrange(p1,p2,p3,p4,p5,p6)
```
As we have already noticed many of the credit card holders pay duly, codes: *-2 and -1* (see Figure: \ref{fig:payment_status}). Majority of the customers do maintain good standing. Noticeably though they **paid the required minimum or more but not the full balance** (code *0*). There is a rather significant group that falls behind with the payment by one or two months.
The next group of features are the bill amounts for the last six months.
```{r bill_density, fig.align="center", fig.cap="Customer Bill Amount Distribution for the Last Six Months", echo=FALSE, fig.height=10}
p1 = ggplot(original, aes(x=BILL_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p2 = ggplot(original, aes(x=BILL_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p3 = ggplot(original, aes(x=BILL_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p4 = ggplot(original, aes(x=BILL_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p5 = ggplot(original, aes(x=BILL_AMT5)) + geom_density(fill=mainPalette[5], colour=mainPalette[5], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p6 = ggplot(original, aes(x=BILL_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
grid.arrange(p1,p2,p3,p4,p5,p6, ncol = 2)
```

The bill payment amounts have negative values, significant amounts that reach at times **thens of thousands** of NT dollars! The negative amount on the credit card bill statements happen when a card holder overpaid his/her bill or were issued a credit after he/she already paid the bill. 

Noticeably the bill amounts have very long tails. They average in tens of thousands of TN dollars, hovering around 50,000 dollars or so on average (see Table: \ref{tab:dataset_summary}). Thus it makes the negative bill amounts we previously observed more plausible.

The last group of the features is the client monthly payments. The data maintained in those columns appears to be integral (see Table: \ref{tab:dataset_summary}). Let's see how the customer payments are distributed. We employ normal and log-scaled visualization for better presentation. 
```{r payment_density, echo=FALSE, fig.align="center", fig.cap="Customer Payment Amount Distribution for the Last Six Months", warning=FALSE, fig.height=8}
p1 = ggplot(original, aes(x=PAY_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p11 = ggplot(original, aes(x=PAY_AMT1)) + geom_density(fill=mainPalette[1], colour=mainPalette[1], alpha = 0.2)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p2 = ggplot(original, aes(x=PAY_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p22 = ggplot(original, aes(x=PAY_AMT2)) + geom_density(fill=mainPalette[2], colour=mainPalette[2], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p3 = ggplot(original, aes(x=PAY_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p33 = ggplot(original, aes(x=PAY_AMT3)) + geom_density(fill=mainPalette[3], colour=mainPalette[3], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p4 = ggplot(original, aes(x=PAY_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p44 = ggplot(original, aes(x=PAY_AMT4)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p5 = ggplot(original, aes(x=PAY_AMT5)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2)+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p55 = ggplot(original, aes(x=PAY_AMT5)) + geom_density(fill=mainPalette[4], colour=mainPalette[4], alpha = 0.2) + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p6 = ggplot(original, aes(x=PAY_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)  +
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
p66 = ggplot(original, aes(x=PAY_AMT6)) + geom_density(fill=mainPalette[6], colour=mainPalette[6], alpha = 0.2)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
grid.arrange(p1,p11,p2,p22,p3,p33,p4,p44,p5,p55,p6,p66, ncol = 2)
```

The pay amounts mirror in the distribution the bill amounts, which is expected. The charts have very long tails which imply that the amounts the card holdrs pay, very greatly. Most likely the payments that are way outside of the normal distribution curve are lump sum payments. More often than not the clients pay between 1000 and 10000 dollars monthly, which is still way below the average bill amount. This finding and the payment status statistics (see Figure: \ref{fig:payment_status}) make us believe that the majority of the credit card hodlders do have quite significant debt, despite the good standing. This hyposesys also explains the distribution of the payments. To keep the debt growth in check the customers pay lump sums whenever they accumulate some saving. Let's plot the delta between the bill amounts and the payment amounts to support our theory. Figure \ref{fig:payment_delta_density}
```{r payment_delta_density, echo=FALSE, fig.align="center", fig.cap="Customer Bill/Payment Amount Delta for Six Months", warning=FALSE}
tmp = original %>% mutate(BILL_PAY_DELTA1 = BILL_AMT1 - PAY_1,BILL_PAY_DELTA2 = BILL_AMT2 - PAY_2,BILL_PAY_DELTA3 = BILL_AMT3 - PAY_3,BILL_PAY_DELTA4 = BILL_AMT4 - PAY_4,BILL_PAY_DELTA5 = BILL_AMT5 - PAY_5, BILL_PAY_DELTA6 = BILL_AMT6 - PAY_6 ) %>% dplyr::select(BILL_PAY_DELTA1,BILL_PAY_DELTA2,BILL_PAY_DELTA3,BILL_PAY_DELTA4,BILL_PAY_DELTA5, BILL_PAY_DELTA6)
tmp = melt(tmp)

ggplot(tmp, aes(x=value, fill=variable, colour = variable)) + 
  geom_density(alpha = 0.1)  + scale_x_log10()+
 theme(axis.text.y=element_blank(),  axis.ticks.y=element_blank(), axis.title.y=element_blank())
rm(p1,p2,p3,p4,p5,p6,p11,p22,p33,p44,p55,p66,tmp)
```

### Data Transformation

Before we proceed further we are going to clean the data set as described in the previous paragraph, namely:

* We will remove *Customer ID* column
* We assign code **4** - **Other** to the *EDUCATION* column values that fall out of the declared code range (1:4)
```{r}
original$EDUCATION = with(original, ifelse(EDUCATION == 0 | EDUCATION == 5 | EDUCATION == 6 , 4, EDUCATION))
data = dplyr::select(original, -ID)
data %>% filter(EDUCATION == 0 | EDUCATION >4) %>% summarise(COUNT=n())
```


### Data Correlation and Principal Component Analysis

In this section of our study we continue exploring the relations between various features of the data set. We put stress on finding the correlatated features, the coreelation between the features and the target label. We also are going to apply principal componenet analysis (PCA) to understand wich attributes of the data set exlain most variance of the original data. If our findings are fruitful we may design a model that requires smaller number of the input parameters without sacrificing the predictive power of the model.

Let's plot the correlation matrix first.
```{r corr_plot, echo=FALSE, fig.align="center", fig.cap="Data Correlation"}
corrplot(cor(select_if(data, is.numeric), use="pairwise.complete.obs"),
         method="color", type="upper",order="hclust", col = mainPalette, number.cex = .4, tl.cex=0.5,
         addCoef.col = "black", tl.col="black", tl.srt=45,sig.level = 0.4, insig = "blank", diag=FALSE )
```

The correlation matrix does not yield any surprises (see Figure: \ref{fig:corr_plot})). Bill payment amounts exhsibit higher correltation as well as the payment status gorup. This does not give us much. The target label has no correlation with any other feature. We proceed with the PCA analysis now. We scale and center the data to achieve meaningful result. We also remove the target feature from the PCA computation. We are going to retain the 15 top components.

```{r pca, echo=FALSE}
dataNOL = dplyr::select(data,-DEFAULT)
dataNOL.pca = prcomp(dataNOL, center = T, scale. = T,  rank. = 15)
print(get_eigenvalue(dataNOL.pca))
```

Good news! The top 10 components explain 83% of the data variance. Let's review what the top four components are made of.

```{r ind_contr, echo=FALSE, fig.align="center", fig.cap="Feature Contribution to the Top Four Components", warning=FALSE}
p1 = fviz_contrib( dataNOL.pca, choice = "var", axes = 1, title = "Comp 1", fill = mainPalette[1], color = mainPalette[1])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p2 = fviz_contrib( dataNOL.pca, choice = "var", axes = 2, title = "Comp 2", fill = mainPalette[2], color = mainPalette[2])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p3 = fviz_contrib( dataNOL.pca, choice = "var", axes = 3, title = "Comp 3", fill = mainPalette[3], color = mainPalette[3])+ theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
p4 = fviz_contrib( dataNOL.pca, choice = "var", axes = 4, title = "Comp 4", fill = mainPalette[4], color = mainPalette[4]) + theme(text = element_text(size = 6),
         axis.title = element_text(size = 6),
         axis.text = element_text(size = 6)
         )
grid.arrange(p1,p2,p3,p4)
```

The red dashed line on the graph (see Figure: \ref{fig:ind_contr}) indicates the expected average contribution. If the contribution of the variables were uniform, the expected value would about 4.3%. For a given component, a variable with a contribution larger than this cutoff could be considered as important in contributing to the component. The PCA analysis supports the correlation matrix (Figure: \ref{fig:corr_plot}) in a sense that the bill amounts and the pay statuses are highly correlated and are subject to the dimentionality reduction due to the redundancy. 

```{r}
fviz_pca_ind(dataNOL.pca,geom.ind = "point", col.ind = as.factor(data$DEFAULT),
             palette = c(mainPalette[1],mainPalette[6]), addEllipses = F,
             legend.title = "Default")
```
 

#### Takeaways from Data Exploration Excersize


## Data Preparation
```{r echo=FALSE}
trainIdx1 <- caret::createDataPartition(y=data[[24]], p=0.7, list=FALSE)
trainData1 <- data[trainIdx1,]

testLRData <-data[-trainIdx1,]

trainIdx2 <- caret::createDataPartition(y=trainData1[[24]], p=0.6, list=FALSE)

trainLRData <- trainData1[trainIdx2,]
validationLRData <- trainData1[-trainIdx2,]


str(trainLRData)

ccTrain <- factorizeData(trainLRData)
ccTrain2 <- normalizeData(ccTrain)
ccTrain3 <- factorizeData(ccTrain2)

ccValidate <- factorizeData(validationLRData)
ccValidate2 <- normalizeData(ccValidate)
ccValidate3 <- factorizeData(ccValidate2)

ccTest <- factorizeData(testLRData)
ccTest2 <- normalizeData(ccTest)
ccTest3 <- factorizeData(ccTest2)
```

### Data Imputing

# Modeling and Evalutation

## Feature Selection


### Data Upsampling


## Naive Bayes Model
Naïve Bayes classification is a kind of simple probabilistic classification methods based on Bayes’ theorem with the assumption of independence between features. 

It is simple (both intuitively and computationally), fast, performs well with small amounts of training data, and scales well to large data sets. The greatest weakness of the naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities. Although this assumption is rarely met, in practice, this algorithm works surprisingly well and accurate; however, on average it rarely can compete with the accuracy of advanced tree-based methods (random forests & gradient boosting machines) but is definitely worth having in our toolkit.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# possible tuning grid
# tuninGrid = data.frame(fL=c(0,0.5,1.0), usekernel = TRUE, adjust=c(0,0.5,1.0))
set.seed(876)
trainDataCopy = mutate(ccTrain3, DEFAULT = as.factor(ifelse(DEFAULT==0, "yes", "no")))
testDataCopy = mutate(ccTest3, DEFAULT = as.factor(ifelse(DEFAULT==0, "yes", "no")))
ctrl = trainControl(method="cv", number = 5, 
    # Estimate class probabilities
    classProbs = T,
    # Evaluate performance using the following function
    summaryFunction = twoClassSummary)

naiveBayesModel = caret::train(as.formula('DEFAULT ~ .'), 
   data = trainDataCopy, method = "nb", metric="ROC", trControl = ctrl)

pred.naiveBayesModel.prob = predict(naiveBayesModel, newdata = testDataCopy, type="prob")
pred.naiveBayesModel.raw = predict(naiveBayesModel, newdata = testDataCopy )

roc.naiveBayesModel = pROC::roc(testDataCopy$DEFAULT, 
                     as.vector(ifelse(pred.naiveBayesModel.prob[,"yes"] >0.5, 1,0)) )
auc.naiveBayesModel = pROC::auc(roc.naiveBayesModel)
naiveBayesModel
```
```{r}
confusionMatrix(data = pred.naiveBayesModel.raw, testDataCopy$DEFAULT)
```
```{r plot_nb_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Naive Bayes Model AUC and ROC Curve", out.width="1.1\\linewidth"}
plot.roc(roc.naiveBayesModel, print.auc = T, auc.polygon = T, col = mainPalette[2] , print.thres = "best" )
```
```{r include=FALSE}
rm(trainDataCopy,testDataCopy,ctrl)
```


## Random Forest Model
Random Forest is also considered as a very handy and easy to use algorithm, because it’s default hyperparameters often produce a good prediction result. Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. The main limitation of Random Forest is that a large number of trees can make the algorithm to slow and ineffective for real-time predictions. In general, these algorithms are fast to train, but quite slow to create predictions once they are trained.
```{r echo=FALSE, message=FALSE, warning=FALSE}

trainDataCopy = mutate(ccTrain3, DEFAULT = as.factor(ifelse(DEFAULT==0, "yes", "no")))
testDataCopy = mutate(ccTest3, DEFAULT = as.factor(ifelse(DEFAULT==0, "yes", "no")))
ctrl = trainControl(method = "cv", number = 3,  
    # Estimate class probabilities
    classProbs = T,
    # Evaluate performance using the following function
    summaryFunction = twoClassSummary)
ptm_rf <- proc.time()
randomForestModel = caret::train(as.formula('DEFAULT ~ .'), 
   data = trainDataCopy, method = "rf", metric="ROC", trControl = ctrl)
proc.time() - ptm_rf
pred.randomForestModel.prob = predict(randomForestModel, newdata = testDataCopy, type="prob")
pred.randomForestModel.raw = predict(randomForestModel, newdata = testDataCopy )

roc.randomForestModel = pROC::roc(testDataCopy$DEFAULT,  
                                  as.vector(ifelse(pred.randomForestModel.prob[,"yes"] >0.5, 1,0)) )
auc.randomForestModel = pROC::auc(roc.randomForestModel)
randomForestModel
```
```{r}
confusionMatrix(data = pred.randomForestModel.raw, testDataCopy$DEFAULT)
```
```{r plot_rf_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Random Forest Model AUC and ROC Curve", out.width="1.1\\linewidth"}
plot.roc(roc.randomForestModel, print.auc = T, auc.polygon = T, col = mainPalette[3] , print.thres = "best" )
```
```{r include=FALSE}
rm(trainDataCopy, testDataCopy,ctrl)
```



## Logistic Regression Model
Logistic regression is an efficient, interpretable and accurate method, which fits quickly with minimal tuning. Logistic regression prediction accuracy will benefit if the data is close to Gaussian distribution. Thus we apply addition transformation to the training data set. We will also be employing 5-fold cross-validation resampling procedure to improve the model.
```{r include=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

ctrl = trainControl(
  # 5-fold CV
  method="cv", number = 5,  
  savePredictions = T)

logRegModel = caret::train(as.formula('DEFAULT ~ .'),
        data = ccTrain3, method="glm", family = binomial(link = "logit"), 
        trControl = ctrl, preProc = c("BoxCox"))

pred.logRegModel.raw = predict(logRegModel, newdata =  ccTest3)
pred.logRegModel.prob = predict(logRegModel, newdata =  ccTest3, type = "prob")
roc.logRegModel = pROC::roc(ccTest3$DEFAULT, as.vector(ifelse(pred.logRegModel.prob[,"1"] >0.5, 1,0)))
auc.logRegModel = pROC::auc(roc.logRegModel)
logRegModel
```
```{r}
confusionMatrix(data = pred.logRegModel.raw, ccTest3$DEFAULT)
```

```{r plot_logReg_ROC,  echo=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.cap="Logistic Regression Model AUC and ROC Curve", out.width="1.1\\linewidth"}
plot.roc(roc.logRegModel, rint.auc = T, auc.polygon = T, col = mainPalette[4] , print.thres = "best" )
```
```{r include=FALSE}
rm(ctrl)
```
Confusion matrix and Figure \ref{fig:plot_logReg_ROC} demonstrate the logistic model performance on the balanced data set. Using the proportion of positive data points that are correctly considered as positive (true positives) and the proportion of negative data points that are mistakenly considered as positive (false negative), we generated a graphic that shows the trade off between the rate at which the model correctly predicts the rain tomorrow with the rate of incorrectly predicting the rain. The value around 0.80 indicates that the model does a good job in discriminating between the two categories.

## Model Comparison
Now it is time to compare the models side by side and pick a winner.

```{r plot_model_comp, fig.align="center", fig.cap="Model AUC Comparison", message=FALSE, warning=FALSE, echo=FALSE, out.width="1.1\\linewidth"}
modelsFace2Face = data.frame(model=c("logRegModel", "naiveBayesModel", "randomForestModel"),
        auc=c(auc.logRegModel, auc.naiveBayesModel, auc.randomForestModel))
modelsFace2Face = modelsFace2Face[order(modelsFace2Face$auc, decreasing = T),]
modelsFace2Face$model = factor(modelsFace2Face$model, levels = modelsFace2Face$model)

ggplot(data = modelsFace2Face, aes(x=model, y=auc)) +
  geom_bar(stat="identity", fill=mainPalette[3], colour=mainPalette[3], alpha = 0.5)

print(modelsFace2Face)
```

#### AUC - ROC perfomance


#### Model interpretibility

Logistic Regression and Naive Bayes are all highly interpreatable models. It is easy to explain to the business what impact each input parameter has. The decision tree could be visualized (provided if it is not too large). 

Random Forest on the other hand is a black-box model, complex algorithm which is difficult to explain in simple terms.


#### Data Preparation
Random Forest and Naive Bayes can deal with missing data, outliers, numeric and alphanumeric values. Simply speaking they are not very demanding for data quality. It would be interesting to see how they perform on the original data set without data cleaning. But this is subject of another research...

Logistic regression does require conversion of alphanumeric values to numeric, struggles dealing with the outliers and performs best when fitted with the data that have normal distribution. 

#### Verdict

Despite sensitivity to data quality Random Forest,Logistic Regression outperforms NaiveBayes model in all other major categories.Both are very close.


# Model Deployment


# Conclusion

\bibliography{RJreferences}

# Note from the Authors

This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.
